{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinanlay/NN/blob/main/4_1_Deep_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WvM4Ikp2Hbgo"
      },
      "id": "WvM4Ikp2Hbgo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Bootcamp @JUIIC\n",
        "\n",
        "# **Notebook 4.1 -- Deep neural networks **\n",
        "\n",
        "(13/12/2023)\n",
        "\n",
        "The purpose of this notebook is to gain some familiarity with deep neural networks with structured data classification for three classes.  It works through an example and experiments with different hyperparameters. <br><br>\n",
        " Change the number of layers, and observe the performance\n"
      ],
      "metadata": {
        "id": "XGSApvmjiyWG"
      },
      "id": "XGSApvmjiyWG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A shallow Neural Network with one hidden layer"
      ],
      "metadata": {
        "id": "IjqZSP56rQn5"
      },
      "id": "IjqZSP56rQn5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b39fac",
      "metadata": {
        "id": "43b39fac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of the ReLU function\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# One-hot encoding for output labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "encoded_y = encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training function\n",
        "def train_neural_network(X_train, y_train, hidden_weights, hidden_bias, output_weights, output_bias, lr, epochs):\n",
        "    for _ in range(epochs):\n",
        "        # Forward Propagation\n",
        "        hidden_layer_activation = np.dot(X_train, hidden_weights)\n",
        "        hidden_layer_activation += hidden_bias\n",
        "        hidden_layer_output = relu(hidden_layer_activation)\n",
        "\n",
        "        output_layer_activation = np.dot(hidden_layer_output, output_weights)\n",
        "        output_layer_activation += output_bias\n",
        "        predicted_output = relu(output_layer_activation)\n",
        "\n",
        "        # Backpropagation\n",
        "        error = y_train - predicted_output\n",
        "        d_predicted_output = error * relu_derivative(predicted_output)\n",
        "\n",
        "        error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
        "        d_hidden_layer = error_hidden_layer * relu_derivative(hidden_layer_output)\n",
        "\n",
        "        # Updating Weights and Biases\n",
        "        output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
        "        output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
        "        hidden_weights += X_train.T.dot(d_hidden_layer) * lr\n",
        "        hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr\n",
        "\n",
        "    return hidden_weights, hidden_bias, output_weights, output_bias, predicted_output\n",
        "\n",
        "# Initialize weights and biases\n",
        "inputLayer_neurons = X_train.shape[1] # number of features in dataset\n",
        "hiddenLayer_neurons = 10 # number of hidden layers neurons\n",
        "outputLayer_neurons = y_train.shape[1] # number of neurons at output layer\n",
        "\n",
        "# Weight and bias initialization\n",
        "hidden_weights = np.random.uniform(size=(inputLayer_neurons, hiddenLayer_neurons))\n",
        "hidden_bias = np.random.uniform(size=(1, hiddenLayer_neurons))\n",
        "output_weights = np.random.uniform(size=(hiddenLayer_neurons, outputLayer_neurons))\n",
        "output_bias = np.random.uniform(size=(1, outputLayer_neurons))\n",
        "\n",
        "# Learning rate and epochs\n",
        "lr = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "# Train the neural network\n",
        "hidden_weights, hidden_bias, output_weights, output_bias, predicted_output = train_neural_network(X_train, y_train, hidden_weights, hidden_bias, output_weights, output_bias, lr, epochs)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "hidden_layer_activation = np.dot(X_test, hidden_weights)\n",
        "hidden_layer_activation += hidden_bias\n",
        "hidden_layer_output = relu(hidden_layer_activation)\n",
        "\n",
        "output_layer_activation = np.dot(hidden_layer_output, output_weights)\n",
        "output_layer_activation += output_bias\n",
        "predicted_output_test = relu(output_layer_activation)\n",
        "\n",
        "# Convert predicted outputs to label indices\n",
        "predicted_labels = np.argmax(predicted_output_test, axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predicted_labels == true_labels)\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Neural network with two hidden layers"
      ],
      "metadata": {
        "id": "TrxJTlF6reoj"
      },
      "id": "TrxJTlF6reoj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2813fe5",
      "metadata": {
        "id": "e2813fe5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of the ReLU function\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "# Evaluation function\n",
        "def evaluate_neural_network(X_test, y_test, weights, biases):\n",
        "    hidden_weights1, hidden_weights2, hidden_weights3, output_weights = weights\n",
        "    hidden_bias1, hidden_bias2, hidden_bias3, output_bias = biases\n",
        "\n",
        "    # Forward propagation on the test set\n",
        "    hidden_layer_activation1 = np.dot(X_test, hidden_weights1)\n",
        "    hidden_layer_activation1 += hidden_bias1\n",
        "    hidden_layer_output1 = relu(hidden_layer_activation1)\n",
        "\n",
        "    hidden_layer_activation2 = np.dot(hidden_layer_output1, hidden_weights2)\n",
        "    hidden_layer_activation2 += hidden_bias2\n",
        "    hidden_layer_output2 = relu(hidden_layer_activation2)\n",
        "\n",
        "    hidden_layer_activation3 = np.dot(hidden_layer_output2, hidden_weights3)\n",
        "    hidden_layer_activation3 += hidden_bias3\n",
        "    hidden_layer_output3 = relu(hidden_layer_activation3)\n",
        "\n",
        "    output_layer_activation = np.dot(hidden_layer_output3, output_weights)\n",
        "    output_layer_activation += output_bias\n",
        "    predicted_output_test = relu(output_layer_activation)\n",
        "\n",
        "    # Convert predicted outputs to label indices\n",
        "    predicted_labels = np.argmax(predicted_output_test, axis=1)\n",
        "    true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    return accuracy\n",
        "\n",
        "# One-hot encoding for output labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "encoded_y = encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training function\n",
        "def train_neural_network(X_train, y_train, weights, biases, lr, epochs):\n",
        "    hidden_weights1, hidden_weights2, hidden_weights3, output_weights = weights\n",
        "    hidden_bias1, hidden_bias2, hidden_bias3, output_bias = biases\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        # Forward Propagation\n",
        "        hidden_layer_activation1 = np.dot(X_train, hidden_weights1)\n",
        "        hidden_layer_activation1 += hidden_bias1\n",
        "        hidden_layer_output1 = relu(hidden_layer_activation1)\n",
        "\n",
        "        hidden_layer_activation2 = np.dot(hidden_layer_output1, hidden_weights2)\n",
        "        hidden_layer_activation2 += hidden_bias2\n",
        "        hidden_layer_output2 = relu(hidden_layer_activation2)\n",
        "\n",
        "        hidden_layer_activation3 = np.dot(hidden_layer_output2, hidden_weights3)\n",
        "        hidden_layer_activation3 += hidden_bias3\n",
        "        hidden_layer_output3 = relu(hidden_layer_activation3)\n",
        "\n",
        "        output_layer_activation = np.dot(hidden_layer_output3, output_weights)\n",
        "        output_layer_activation += output_bias\n",
        "        predicted_output = relu(output_layer_activation)\n",
        "\n",
        "        # Backpropagation\n",
        "        error = y_train - predicted_output\n",
        "        d_predicted_output = error * relu_derivative(predicted_output)\n",
        "\n",
        "        error_hidden_layer3 = d_predicted_output.dot(output_weights.T)\n",
        "        d_hidden_layer3 = error_hidden_layer3 * relu_derivative(hidden_layer_output3)\n",
        "\n",
        "        error_hidden_layer2 = d_hidden_layer3.dot(hidden_weights3.T)\n",
        "        d_hidden_layer2 = error_hidden_layer2 * relu_derivative(hidden_layer_output2)\n",
        "\n",
        "        error_hidden_layer1 = d_hidden_layer2.dot(hidden_weights2.T)\n",
        "        d_hidden_layer1 = error_hidden_layer1 * relu_derivative(hidden_layer_output1)\n",
        "\n",
        "        # Updating Weights and Biases\n",
        "        output_weights += hidden_layer_output3.T.dot(d_predicted_output) * lr\n",
        "        output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
        "\n",
        "        hidden_weights3 += hidden_layer_output2.T.dot(d_hidden_layer3) * lr\n",
        "        hidden_bias3 += np.sum(d_hidden_layer3, axis=0, keepdims=True) * lr\n",
        "\n",
        "        hidden_weights2 += hidden_layer_output1.T.dot(d_hidden_layer2) * lr\n",
        "        hidden_bias2 += np.sum(d_hidden_layer2, axis=0, keepdims=True) * lr\n",
        "\n",
        "        hidden_weights1 += X_train.T.dot(d_hidden_layer1) * lr\n",
        "        hidden_bias1 += np.sum(d_hidden_layer1, axis=0, keepdims=True) * lr\n",
        "\n",
        "    return (hidden_weights1, hidden_weights2, hidden_weights3, output_weights), (hidden_bias1, hidden_bias2, hidden_bias3, output_bias), predicted_output\n",
        "\n",
        "# Initialize weights and biases for 3 hidden layers\n",
        "inputLayer_neurons = X_train.shape[1] # number of features in dataset\n",
        "hiddenLayer_neurons1 = 10 # number of neurons in first hidden layer\n",
        "hiddenLayer_neurons2 = 8 # number of neurons in second hidden layer\n",
        "hiddenLayer_neurons3 = 6 # number of neurons in third hidden layer\n",
        "outputLayer_neurons = y_train.shape[1] # number of neurons at output layer\n",
        "\n",
        "# Weight and bias initialization for 3 hidden layers\n",
        "hidden_weights1 = np.random.uniform(size=(inputLayer_neurons, hiddenLayer_neurons1))\n",
        "hidden_bias1 = np.random.uniform(size=(1, hiddenLayer_neurons1))\n",
        "hidden_weights2 = np.random.uniform(size=(hiddenLayer_neurons1, hiddenLayer_neurons2))\n",
        "hidden_bias2 = np.random.uniform(size=(1, hiddenLayer_neurons2))\n",
        "hidden_weights3 = np.random.uniform(size=(hiddenLayer_neurons2, hiddenLayer_neurons3))\n",
        "hidden_bias3 = np.random.uniform(size=(1, hiddenLayer_neurons3))\n",
        "output_weights = np.random.uniform(size=(hiddenLayer_neurons3, outputLayer_neurons))\n",
        "output_bias = np.random.uniform(size=(1, outputLayer_neurons))\n",
        "\n",
        "# Learning rate and epochs\n",
        "lr = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "# Train the neural network\n",
        "weights = (hidden_weights1, hidden_weights2, hidden_weights3, output_weights)\n",
        "biases = (hidden_bias1, hidden_bias2, hidden_bias3, output_bias)\n",
        "weights, biases, predicted_output = train_neural_network(X_train, y_train, weights, biases, lr, epochs)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = evaluate_neural_network(X_test, y_test, weights, biases)\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Three hidden layer neural network using Keras"
      ],
      "metadata": {
        "id": "RwPFd29Ar5B0"
      },
      "id": "RwPFd29Ar5B0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece59906",
      "metadata": {
        "id": "ece59906"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding for output labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_y = encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model with three hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu')) # First hidden layer\n",
        "model.add(Dense(8, activation='relu')) # Second hidden layer\n",
        "model.add(Dense(6, activation='relu')) # Third hidden layer\n",
        "model.add(Dense(y_train.shape[1], activation='softmax')) # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test set accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Plot the training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A three hidden layer neural network using Keras, with confusion matrix display"
      ],
      "metadata": {
        "id": "5sC9bmY4sL-Q"
      },
      "id": "5sC9bmY4sL-Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac84a6a",
      "metadata": {
        "id": "cac84a6a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding for output labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_y = encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model with three hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu')) # First hidden layer\n",
        "model.add(Dense(8, activation='relu')) # Second hidden layer\n",
        "model.add(Dense(6, activation='relu')) # Third hidden layer\n",
        "model.add(Dense(y_train.shape[1], activation='softmax')) # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test set accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Predict the test set results\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training history (same as before)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A neural Network with hidden layer; with display of confusion matrix and ROC"
      ],
      "metadata": {
        "id": "uqDbox7bsl-a"
      },
      "id": "uqDbox7bsl-a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dae1eaa",
      "metadata": {
        "id": "9dae1eaa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding for output labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_y = encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model with three hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu')) # First hidden layer\n",
        "model.add(Dense(8, activation='relu')) # Second hidden layer\n",
        "model.add(Dense(6, activation='relu')) # Third hidden layer\n",
        "model.add(Dense(y_train.shape[1], activation='softmax')) # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test set accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Predict the test set results\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "n_classes = y_train.shape[1]\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure(figsize=(8, 6))\n",
        "lw = 2\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "# Plot micro-average ROC curve\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "# Plot ROC curve for a random classifier\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic to multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "1.   Change the number of hidden layers to four and see the prediction accuracy\n",
        "2.   Double the number of hidden units in the first hidden layer and observe the impact on accuacy\n",
        "\n"
      ],
      "metadata": {
        "id": "OWkv1xh7tb3S"
      },
      "id": "OWkv1xh7tb3S"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}